ğŸ” What is MLflow?
MLflow is an open-source platform to manage the end-to-end machine learning lifecycle, including:

âœ… Experiment tracking
âœ… Model packaging
âœ… Model deployment
âœ… Model registry`

Itâ€™s framework-agnostic â†’ works with TensorFlow, PyTorch, Scikit-learn, XGBoost, etc.

ğŸš€ Core Capabilities & Components
MLflow has 4 main components:

1ï¸âƒ£ MLflow Tracking
Tracks experiments, parameters, metrics, artifacts (models, plots, files)
â†’ Logs results in a centralized server
â†’ Web UI & API to compare experiments

â­ Key features:

Log code versions, parameters, metrics, artifacts

View & compare runs across models & datasets

2ï¸âƒ£ MLflow Projects
Helps package ML code in a reproducible format
â†’ Uses conda or docker environments to encapsulate dependencies

â­ Key features:

Standard YAML format (MLproject file)

Runs code in different environments easily

Supports GitHub integration

3ï¸âƒ£ MLflow Models
Provides a standardized model packaging format
â†’ Package models for multiple serving environments

â­ Key features:

Save models as flavors: TensorFlow, PyTorch, sklearn, XGBoost, etc.

Export models for REST API, batch scoring, cloud serving (AWS Sagemaker, AzureML)

4ï¸âƒ£ MLflow Model Registry
A central repository to manage model versions
â†’ Enables staging, production, archiving, and annotation of models

â­ Key features:

Model versioning

Lifecycle transitions: Staging â†’ Production

Approval workflows

Track model metadata

ğŸ† Other Notable Features:
âœ… Extensible â†’ Custom plugins, tracking servers
âœ… Scalable â†’ Works locally, on-prem, or cloud
âœ… Supports REST API & Python, R, Java clients
âœ… Integrated with Databricks, AWS, Azure, GCP

ğŸ¯ Why Use MLflow?
âœ”ï¸ Centralized tracking of experiments
âœ”ï¸ Reproducible training & deployment pipelines
âœ”ï¸ Easy collaboration between data scientists, engineers, ops
âœ”ï¸ Streamlined CI/CD for ML models

In short â†’ MLflow bridges the gap between ML development and production by offering experimentation, reproducibility, model management, and deployment in one platform.


âœ… What Can Be Logged in MLflow Tracking?
MLflow allows logging 5 main types of data:

1ï¸âƒ£ Parameters
â†’ Key-value pairs of inputs to your ML experiment
ğŸ“ Example:

python
Copy
Edit
mlflow.log_param("learning_rate", 0.01)
mlflow.log_param("max_depth", 5)
ğŸ‘‰ Typically small, scalar values â†’ hyperparameters, configurations

2ï¸âƒ£ Metrics
â†’ Numerical performance indicators tracked over time or at end
ğŸ“ Example:

python
Copy
Edit
mlflow.log_metric("accuracy", 0.92)
mlflow.log_metric("loss", 0.05, step=10)
ğŸ‘‰ Supports single value or multiple values per metric (tracked over steps/epochs)

3ï¸âƒ£ Artifacts
â†’ Output files generated by your code (can be any file: plots, models, logs, configs)
ğŸ“ Example:

python
Copy
Edit
mlflow.log_artifact("confusion_matrix.png")
mlflow.log_artifacts("models/")  # logs entire directory
ğŸ‘‰ Stored in artifact storage (local, S3, GCS, etc.)

4ï¸âƒ£ Source Code Versions
â†’ Logs Git commit hash or code snapshot
âœ”ï¸ Ensures reproducibility by tying a run to the code version

ğŸ“ Happens automatically if Git repo detected (or manually via API)

5ï¸âƒ£ Models
â†’ Log trained ML models in standard format with flavors
ğŸ“ Example:

python
Copy
Edit
mlflow.sklearn.log_model(model, "model")
ğŸ‘‰ Enables model serving, deployment, versioning

ğŸ† Additional Notes:
ğŸ”¹ Logging supports multiple programming languages â†’ Python, R, Java, REST API
ğŸ”¹ Can log custom metadata tags â†’ mlflow.set_tag("team", "ds_team")
ğŸ”¹ All logged data is associated with a run ID inside an experiment

ğŸ‘‰ In summary:
MLflow can log
âœ… Inputs (parameters)
âœ… Outputs (metrics, artifacts)
âœ… Code context (version)
âœ… Models
âœ… Metadata (tags)