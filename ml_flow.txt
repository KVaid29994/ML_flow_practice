🔍 What is MLflow?
MLflow is an open-source platform to manage the end-to-end machine learning lifecycle, including:

✅ Experiment tracking
✅ Model packaging
✅ Model deployment
✅ Model registry`

It’s framework-agnostic → works with TensorFlow, PyTorch, Scikit-learn, XGBoost, etc.

🚀 Core Capabilities & Components
MLflow has 4 main components:

1️⃣ MLflow Tracking
Tracks experiments, parameters, metrics, artifacts (models, plots, files)
→ Logs results in a centralized server
→ Web UI & API to compare experiments

⭐ Key features:

Log code versions, parameters, metrics, artifacts

View & compare runs across models & datasets

2️⃣ MLflow Projects
Helps package ML code in a reproducible format
→ Uses conda or docker environments to encapsulate dependencies

⭐ Key features:

Standard YAML format (MLproject file)

Runs code in different environments easily

Supports GitHub integration

3️⃣ MLflow Models
Provides a standardized model packaging format
→ Package models for multiple serving environments

⭐ Key features:

Save models as flavors: TensorFlow, PyTorch, sklearn, XGBoost, etc.

Export models for REST API, batch scoring, cloud serving (AWS Sagemaker, AzureML)

4️⃣ MLflow Model Registry
A central repository to manage model versions
→ Enables staging, production, archiving, and annotation of models

⭐ Key features:

Model versioning

Lifecycle transitions: Staging → Production

Approval workflows

Track model metadata

🏆 Other Notable Features:
✅ Extensible → Custom plugins, tracking servers
✅ Scalable → Works locally, on-prem, or cloud
✅ Supports REST API & Python, R, Java clients
✅ Integrated with Databricks, AWS, Azure, GCP

🎯 Why Use MLflow?
✔️ Centralized tracking of experiments
✔️ Reproducible training & deployment pipelines
✔️ Easy collaboration between data scientists, engineers, ops
✔️ Streamlined CI/CD for ML models

In short → MLflow bridges the gap between ML development and production by offering experimentation, reproducibility, model management, and deployment in one platform.


✅ What Can Be Logged in MLflow Tracking?
MLflow allows logging 5 main types of data:

1️⃣ Parameters
→ Key-value pairs of inputs to your ML experiment
📝 Example:

python
Copy
Edit
mlflow.log_param("learning_rate", 0.01)
mlflow.log_param("max_depth", 5)
👉 Typically small, scalar values → hyperparameters, configurations

2️⃣ Metrics
→ Numerical performance indicators tracked over time or at end
📝 Example:

python
Copy
Edit
mlflow.log_metric("accuracy", 0.92)
mlflow.log_metric("loss", 0.05, step=10)
👉 Supports single value or multiple values per metric (tracked over steps/epochs)

3️⃣ Artifacts
→ Output files generated by your code (can be any file: plots, models, logs, configs)
📝 Example:

python
Copy
Edit
mlflow.log_artifact("confusion_matrix.png")
mlflow.log_artifacts("models/")  # logs entire directory
👉 Stored in artifact storage (local, S3, GCS, etc.)

4️⃣ Source Code Versions
→ Logs Git commit hash or code snapshot
✔️ Ensures reproducibility by tying a run to the code version

📝 Happens automatically if Git repo detected (or manually via API)

5️⃣ Models
→ Log trained ML models in standard format with flavors
📝 Example:

python
Copy
Edit
mlflow.sklearn.log_model(model, "model")
👉 Enables model serving, deployment, versioning

🏆 Additional Notes:
🔹 Logging supports multiple programming languages → Python, R, Java, REST API
🔹 Can log custom metadata tags → mlflow.set_tag("team", "ds_team")
🔹 All logged data is associated with a run ID inside an experiment

👉 In summary:
MLflow can log
✅ Inputs (parameters)
✅ Outputs (metrics, artifacts)
✅ Code context (version)
✅ Models
✅ Metadata (tags)